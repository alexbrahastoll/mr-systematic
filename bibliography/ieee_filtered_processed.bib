@inproceedings{8811961,
  author = {{Atlidakis}, V. and {Godefroid}, P. and {Polishchuk}, M.},
  booktitle = {2019 IEEE/ACM 41st International Conference on Software Engineering (ICSE)},
  title = {RESTler: Stateful REST API Fuzzing},
  year = {2019},
  volume = {},
  number = {},
  pages = {748-758},
  abstract = {This paper introduces RESTler, the first stateful REST API fuzzer. RESTler analyzes the API specification of a cloud service and generates sequences of requests that automatically test the service through its API. RESTler generates test sequences by (1) inferring producer-consumer dependencies among request types declared in the specification (e.g., inferring that "a request B should be executed after request A" because B takes as an input a resource-id x produced by A) and by (2) analyzing dynamic feedback from responses observed during prior test executions in order to generate new tests (e.g., learning that "a request C after a request sequence A;B is refused by the service" and therefore avoiding this combination in the future). We present experimental results showing that these two techniques are necessary to thoroughly exercise a service under test while pruning the large search space of possible request sequences. We used RESTler to test GitLab, an open-source Git service, as well as several Microsoft Azure and Office365 cloud services. RESTler found 28 bugs in GitLab and several bugs in each of the Azure and Office365 cloud services tested so far. These bugs have been confirmed and fixed by the service owners.},
  keywords = {application program interfaces;cloud computing;fuzzy set theory;program debugging;program testing;prior test executions;request C;test GitLab;open-source Git service;Office365 cloud services;service owners;stateful REST API fuzzing;stateful REST API fuzzer;RESTler analyzes;API specification;cloud service;test sequences;request types;request B;request sequences;Microsoft Azure cloud services;bugs;Computer bugs;Tools;Fuzzing;Dictionaries;Open source software;Test pattern generators;REST API;Fuzzing;cloud services;fuzzer;testing;bug finding},
  doi = {10.1109/ICSE.2019.00083},
  issn = {1558-1225},
  month = may,
  month_numeric = {5},
  mrs_inclusion_criteria={all},
}
@inproceedings{8029755,
  author = {{Serrano}, D. and {Stroulia}, E. and {Lau}, D. and {Ng}, T.},
  booktitle = {2017 IEEE International Conference on Web Services (ICWS)},
  title = {Linked REST APIs: A Middleware for Semantic REST API Integration},
  year = {2017},
  volume = {},
  number = {},
  pages = {138-145},
  abstract = {Over the last decade, an exponentially increasing number of REST services have been providing a simple and straightforward syntax for accessing rich data resources. To use these services, however, developers have to understand "information-use contracts" specified in natural language, and, to build applications that benefit from multiple existing services they have to map the underlying resource schemas in their code. This process is difficult and error-prone, especially as the number and overlap of the underlying services increases, and the mappings become opaque, difficult to maintain, and practically impossible to reuse. The more recent advent of the Linked Data formalisms can offer a solution to the challenge. In this paper, we propose a conceptual framework for REST-service integration based on Linked Data models. In this framework, the data exposed by REST services is mapped to Linked Data schemas, based on these descriptions, we have developed a middleware that can automatically compose API calls to respond to data queries (in SPARQL). Furthermore, we have developed a RDF model for characterizing the access-control protocols of these APIs and the quality of the data they expose, so that our middleware can develop "legal" compositions with desired qualities. We report our experience with the implementation of a prototype that demonstrates the usefulness of our framework in the context of a research-data management application.},
  keywords = {application program interfaces;authorisation;data integration;Linked Data;middleware;query processing;semantic Web;SQL;Linked REST API;middleware;semantic REST API integration;information-use contracts;Linked Data formalisms;REST-service integration;Linked Data models;API calls;data queries;SPARQL;RDF model;access-control protocols;data quality;legal compositions;research-data management;Semantics;Web services;Ontologies;Data models;Syntactics;data integration;Linked Data;REST APIs},
  doi = {10.1109/ICWS.2017.26},
  issn = {},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, api_spec, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose},
}
@inproceedings{8421389,
  author = {{Di Martino}, B. and {Posillipo}, A. and {Nacchia}, S. and {Maisto}, S. A.},
  booktitle = {2018 IEEE International Conference on Smart Computing (SMARTCOMP)},
  title = {A Q A Tool to Produce an Ad-Hoc OpenAPI Specification to Identify Equivalent REST Api Services},
  year = {2018},
  volume = {},
  number = {},
  pages = {375-380},
  abstract = {The highly heterogeneous environment of IoT (Internet of Things) and Smart cities guarantees that every stakeholder's requirements can be achieved and implemented. However deciding which is the most appropriate sensor to adopt among various similar ones and, even most important, how to use it is not an easy task; a task that, as of today, cannot be done without access to source code or documentation. Moreover heterogeneity in descriptions and definitions complicate sensor interfaces comparison and selection, and may create interoperability and portability problems among multiple providers. In this work we aim at providing a Q&A tool that guides the user in the production of a RESTApi sensor's interfaces according to the OpenAPI specification. The automatically produced ad-hoc specification is used as a base for defining sensors' interfaces equivalence and similarity using natural language processing techniques.},
  keywords = {ad hoc networks;application program interfaces;Internet of Things;natural language processing;open systems;question answering (information retrieval);sensors;smart cities;interoperability;portability problems;Q&A tool;IoT;Internet of things;smart cities;stakeholders requirement;natural language processing techniques;RESTApi sensor interface;application program interface;equivalent REST Api services identification;ad-hoc OpenAPI specification;Tools;Documentation;Standards;Electronic mail;Task analysis;Natural language processing;Servers;IoT;Rest API;OpenAPI Specification;Q&A Tool;NLP;Text similarity;Natural Language Processing},
  doi = {10.1109/SMARTCOMP.2018.00032},
  issn = {},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, api_spec, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose},
}
@inproceedings{6009431,
  author = {{Li}, L. and {Chou}, W.},
  booktitle = {2011 IEEE International Conference on Web Services},
  title = {Design and Describe REST API without Violating REST: A Petri Net Based Approach},
  year = {2011},
  volume = {},
  number = {},
  pages = {508-515},
  abstract = {As REST architectural style gains popularity in the web service community, there is a growing concern and debate on how to design Restful web services (REST API) in a proper way. We attribute this problem to lack of a standard model and language to describe a REST API that respects all the REST constraints. As a result, many web services that claim to be REST API are not hypermedia driven as prescribed by REST. This situation may lead to REST APIs that are not as scalable, extensible, and interoperable as promised by REST. To address this issue, this paper proposes REST Chart as a model and language to design and describe REST API without violating the REST constraints. REST Chart models a REST API as a special type of Colored Petri Net whose topology defines the REST API and whose token markings define the representational state space of user agents using that API. We demonstrate REST Chart with an example REST API. We also show how REST Chart can support efficient content negotiation and reuse hybrid representations to broaden design choices. Furthermore, we argue that the REST constraints, such as hypermedia driven and statelessness, can either be enforced naturally or checked automatically in REST Chart.},
  keywords = {application program interfaces;Petri nets;Web services;REST API;RESTful Web services;REST constraints;REST chart models;colored Petri net;Media;XML;Web services;Servers;Protocols;Unified modeling language;HTML;RESTful web service;Petri Net;REST Chart},
  doi = {10.1109/ICWS.2011.54},
  issn = {},
  month = jul,
  month_numeric = {7},
  mrs_inclusion_criteria={full_version, recent, rest_api, api_spec, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose},
}
@inproceedings{8440193,
  author = {{Agocs}, A. and {Goff}, J. L.},
  booktitle = {2018 International Conference on Computer, Information and Telecommunication Systems (CITS)},
  title = {A web service based on RESTful API and JSON Schema/JSON Meta Schema to construct knowledge graphs},
  year = {2018},
  volume = {},
  number = {},
  pages = {1-5},
  abstract = {Data visualisation assists domain experts in understanding their data and helps them make critical decisions. Enhancing their cognitive insight essentially relies on the capability of combining domain-specific semantic information with concepts extracted out of the data and visualizing the resulting networks. Data scientists have the challenge of providing tools able to handle the overall network lifecycle. In this paper, we present how the combination of two powerful technologies namely the REST architecture style and JSON Schema/JSON Meta Schema enable data scientists to use a RESTful web service that permits the construction of knowledge graphs, one of the preferred representations of large and semantically rich networks.},
  keywords = {application program interfaces;data analysis;data visualisation;software architecture;Web services;REST architecture style;network lifecycle;RESTful Web service;RESTful API;JSON Schema/JSON Meta Schema;domain-specific semantic information;data visualisation;knowledge graphs;Service-oriented architecture;Collaboration;Databases;Data visualization;Ontologies;Computer architecture;RESTful API;JSON;JSON Schema;Data validation},
  doi = {10.1109/CITS.2018.8440193},
  issn = {},
  month = jul,
  month_numeric = {7},
  mrs_inclusion_criteria={full_version, recent, rest_api, api_spec, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose},
}
@inproceedings{8356445,
  author = {{Stoudenmier}, S. and {Olmsted}, A.},
  booktitle = {2017 12th International Conference for Internet Technology and Secured Transactions (ICITST)},
  title = {Efficient retrieval of information from hierarchical REST requests},
  year = {2017},
  volume = {},
  number = {},
  pages = {452-454},
  abstract = {REST requests are utilized by developers across many fields to access public data that they would not normally have access to. Typically, efficiency is not a concern when making a REST request to a server since only a handful are performed at once. With the increasing popularity of gaming, more and more web services have emerged that aim to present the data taken from these APIs. API Keys and other methods of authorization used are associated with an account that has a maximum number of requests that are allowed per a specific unit of time. The problem is that these databases are relational and result in multiple REST requests to traverse the hierarchical structure and retrieve the needed information, leading to a long runtime before the application is loaded. As a result, many of these web services have downtime at specific intervals, such as the release of a new patch, so that all necessary information can be retrieved, stored on a local server, and then presented to users. This paper presents a solution to increase the efficiency of retrieving data from a REST API such that a web service that presents this information may minimize its downtime.},
  keywords = {application program interfaces;authorisation;information retrieval;Web services;hierarchical REST requests;web service;API Keys;multiple REST requests;hierarchical structure;REST API;public data access;efficient information retrieval;relational databases;authorization;Web services;Servers;Authorization;Industries;Games;Databases;REST;API;API keys;big data;League of Legends},
  doi = {10.23919/ICITST.2017.8356445},
  issn = {},
  month = dec,
  month_numeric = {12},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7562096,
  author = {{Rivera}, S. and {Fei}, Z. and {Griffioen}, J.},
  booktitle = {2016 IEEE Conference on Computer Communications Workshops (INFOCOM WKSHPS)},
  title = {RAPTOR: A REST API translaTOR for OpenFlow controllers},
  year = {2016},
  volume = {},
  number = {},
  pages = {328-333},
  abstract = {Many Software-Defined Networking (SDN) controllers support a “northbound interface” by which applications can interact with the SDN controller and (indirectly) control the underlying SDN network. The absence of a standard for the northbound interface of these controllers makes it difficult for application developers to create interoperable/portable code (i.e., code that works with any SDN controller). Developers are forced to re-write almost all of their code every time they want to support a new controller. This tedious and time consuming process is typically a start-over software development cycle that involves learning new APIs, data models, and controller-specific conventions. In this paper, we present RAPTOR, a REST-based API translaTOR service for SDN networks that allows users to develop their network control software independent of any particular SDN controller. RAPTOR exposes its own REST-based API functions and data models to user applications and translates application requests into controller-specific northbound interface calls. To demonstrate the viability of RAPTOR, we implemented and deployed RAPTOR in GENI and used it in conjunction with different types of controllers. We also developed GENI Desktop modules that interacted with RAPTOR to install, list, delete and monitor end-to-end flows regardless of the controller used for controlling the underlying switches.},
  keywords = {application program interfaces;data models;learning (artificial intelligence);protocols;software defined networking;RAPTOR;REST API translator;OpenFlow controllers;software-defined networking controllers;SDN controller;SDN network;interoperable/portable code;software development cycle;API learning;data models;controller-specific conventions;network control software;REST-based API functions;controller-specific northbound interface calls;GENI desktop modules;end-to-end flow installation;end-to-end flow listing;end-to-end flow deletion;end-to-end flow monitoring;Data models;Control systems;Media;Standards;Software;Electronic mail;Servers},
  doi = {10.1109/INFCOMW.2016.7562096},
  issn = {},
  month = apr,
  month_numeric = {4},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@article{6712032,
  author = {},
  journal = {IEEE Std 1874-2013},
  title = {IEEE Standard for Documentation Schema for Repair and Assembly of Electronic Devices},
  year = {2014},
  volume = {},
  number = {},
  pages = {1-42},
  abstract = {oManual is a standard for storing and transmitting procedural manuals. oManuals common data format can be used as an offline file package or via online RESTful API endpoints, using XML or JSON. This format is useful for documenting and describing repairs, how-to, workinstructions, or any other step-by-step guides. oManual makes it easy to exchange procedural information between services while maintaining usability on mobile devices.This specification describes the oManual data model, web services API, and bundle file format (a collection of structured files containing a category XML format, a guide XML format and related multimedia). The specification may be expanded in the future to enable additional types of documents.},
  keywords = {application program interfaces;assembling;electronic data interchange;electronic engineering computing;file organisation;IEEE standards;Java;maintenance engineering;software packages;user manuals;Web services;XML;bundle file format;Web services;oManual data model;mobile devices;procedural information exchange;JSON;XML;online RESTful API endpoints;offline file package;data format;procedural manuals;documentation schema;electronic device repair;electronic device assembly;IEEE Std 1874-2013;IEEE standards;Maintenance engineering;Equipment maintenance;IEEE 1874(TM);JSON;manual;oEmbed;oManual;RESTful API;XML;ZIP},
  doi = {10.1109/IEEESTD.2014.6712032},
  issn = {},
  month = jan,
  month_numeric = {1},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{6906841,
  author = {{Sellami}, R. and {Bhiri}, S. and {Defude}, B.},
  booktitle = {2014 IEEE International Congress on Big Data},
  title = {ODBAPI: A Unified REST API for Relational and NoSQL Data Stores},
  year = {2014},
  volume = {},
  number = {},
  pages = {653-660},
  abstract = {Cloud computing has recently emerged as a new computing paradigm enabling on-demand and scalable provision of resources, platforms and software as services. In order to satisfy different storage requirements, cloud applications usually need to access and interact with different relational and NoSQL data stores having heterogeneous APIs. This APIs heterogeneity induces two main problems. First it ties cloud applications to specific data stores hampering therefore their migration. Second, it requires developers to be familiar with different APIs. In this paper, we propose a generic resources model defining the different concept used in each type of data store. These resources are managed by ODBAPI a streamlined and a unified REST API enabling to execute CRUD operations on different NoSQL and relational databases. ODBAPI decouples cloud applications from data stores alleviating therefore their migration. Moreover it relieves developers task by removing the burden of managing different APIs.},
  keywords = {application program interfaces;cloud computing;relational databases;SQL;ODBAPI;unified REST API;NoSQL data stores;relational data stores;cloud computing;heterogeneous API;generic resources model;CRUD operations;Databases;Data models;Cloud computing;Database languages;Context;Computational modeling;REST-based API;NoSQL data stores;relational data stores;CRUD operations},
  doi = {10.1109/BigData.Congress.2014.98},
  issn = {2379-7703},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7195624,
  author = {{Li}, L. and {Chou}, W.},
  booktitle = {2015 IEEE International Conference on Web Services},
  title = {Designing Large Scale REST APIs Based on REST Chart},
  year = {2015},
  volume = {},
  number = {},
  pages = {631-638},
  abstract = {REST Chart is a Petri-Net based XML modeling framework for REST API. This paper presents two important enhancements and extensions to REST Chart modeling - Hyperlink Decoration and Hierarchical REST Chart. In particular, the proposed Hyperlink Decoration decomposes resource connections from resource representation, such that hyperlinks can be defined independently of schemas. This allows a Navigation-First Design by which the important global connections of a REST API can be designed first and reused before the local resource representations are implemented and specified. Hierarchical REST Chart is a powerful mechanism to rapidly decompose and extend a REST API in several dimensions based on Hyperlink Decoration. These new mechanisms can be used to manage the complexities in large scale REST APIs that undergo frequent changes as in some large scale open source development projects. This paper shows that these new capabilities can fit nicely in the REST Chart XML with very minor syntax changes. These enhancements to REST Chart are applied successfully in designing and verifying REST APIs for software-defined-networking (SDN) and Cloud computing.},
  keywords = {application program interfaces;charts;cloud computing;Petri nets;software defined networking;XML;REST API;REST Chart modelling;Petri net based XML modelling framework;hyperlink decoration;software-defined-networking;SDN;cloud computing;XML;Media;Protocols;Navigation;Fires;Web services;Testing;REST API;REST Chart;Petri-Net;hyperlink decoration;hierarchical Petri-Net;service description language;XML},
  doi = {10.1109/ICWS.2015.89},
  issn = {},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, api_spec, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose},
}
@inproceedings{7125541,
  author = {{Harcuba}, O. and {Vrba}, P.},
  booktitle = {2015 IEEE International Conference on Industrial Technology (ICIT)},
  title = {Unified REST API for supporting the semantic integration in the ESB-based architecture},
  year = {2015},
  volume = {},
  number = {},
  pages = {3000-3005},
  abstract = {This paper presents the framework for integrating the lightweight clients with the heavyweight Enterprise Service Bus (ESB) messaging system via the HTTP-based RESTful API. A special gateway converts the ESB messages, which content is encoded in RDF according to the OWL ontology to standard HTTP requests with the JSON content and vice versa. Such a solution is developed within the European ARUM project aimed at the optimization of production ramp-up of highly complex and customized products. The framework is used to simplify the integration of user interfaces for production monitoring and scheduling running on a smart phone or in a Web browser with the backbone ESB infrastructure that hosts the core computational components such as schedulers, planners, and optimizers.},
  keywords = {application program interfaces;multi-agent systems;ontologies (artificial intelligence);scheduling;service-oriented architecture;user interfaces;Web services;semantic integration;ESB-based architecture;enterprise service bus messaging system;ESB message;HTTP-based RESTful API;application program interface;RDF;OWL ontology;JSON content;production ramp-up optimization;user interface;production monitoring;scheduling;service-oriented architecture;SOA;REST based Web service;multiagent system;Ontologies;Job shop scheduling;Logic gates;Uniform resource locators;Resource description framework;OWL;multi-agent systems;Service Oriented Architectures;Enterprise Service Bus;REST;JBossESB;industrial automation},
  doi = {10.1109/ICIT.2015.7125541},
  issn = {},
  month = mar,
  month_numeric = {3},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8776964,
  author = {{Jain}, H. and {Kakkar}, M.},
  booktitle = {2019 9th International Conference on Cloud Computing, Data Science Engineering (Confluence)},
  title = {Job Recommendation System based on Machine Learning and Data Mining Techniques using RESTful API and Android IDE},
  year = {2019},
  volume = {},
  number = {},
  pages = {416-421},
  abstract = {In the current Capitalist world with an abundance of different state-of-the-art industries and fields cropping up, ushering in an influx of jobs for motivated and talented professionals, it is not difficult to identify your field and to persevere to get a job in the respective field but lack of information and awareness render the task difficult. This problem is being tackled by Job Recommendation systems. But not every aspect from the wide spectrum of factors is incorporated in the existing systems. For the "Job Recommendation System - Vitae" machine learning and data mining techniques were applied to a RESTful Web Server application that bridges the gap between the Frontend (Android Application) and the Backend (MongoDB instance) using APIs. The data communicated through APIs is fed into the database and the Recommendation System uses that data to synthesize the results. To make the existing systems even more reliable, here efforts have been done to come up with the idea of a system that uses a wide variety of factors and is not only a one-way recommendation system.},
  keywords = {Android (operating system);application program interfaces;business data processing;data mining;employment;Internet;learning (artificial intelligence);NoSQL databases;programming environments;recommender systems;data mining techniques;RESTful Web Server application;job recommendation system;machine learning;RESTful APIs;Android IDE;MongoDB instance;Industries;Filtering;Databases;Web servers;Companies;Collaboration;Machine learning;Data mining;Machine Learning;Recommendation Systems;Content Based filtering;RESTful API;Android Application;Job Recommendations},
  doi = {10.1109/CONFLUENCE.2019.8776964},
  issn = {},
  month = jan,
  month_numeric = {1},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7382981,
  author = {{Zaslavskiy}, M. and {Mouromtsev}, D.},
  booktitle = {2015 Artificial Intelligence and Natural Language and Information Extraction, Social Media and Web Search FRUCT Conference (AINL-ISMW FRUCT)},
  title = {Implementation of the new REST API for open source LBS-platform Geo2Tag},
  year = {2015},
  volume = {},
  number = {},
  pages = {125-130},
  abstract = {The article describes current state of Geo2Tag LBS platform project and new API version implementation. The platform was improved by following challenges: data visualization, extended datetime processing, social network integration and background calculations support. These challenges were justified by review of most important tendencies for geocontext applications and LBS platforms. Recommendations were fully implemented in API. Also the article contains description of new version implementation. As an example Open Data import API and specific plugin for Open Karelia system was implemented. This extension allowed performing geocontext markup of complex spatiotemporal data inside the platform.},
  keywords = {application program interfaces;data visualisation;mobile computing;public domain software;social networking (online);REST API;open source LBS-platform Geo2Tag project;API version implementation;data visualization;extended datetime processing;social network integration;background calculation support;geocontext applications;Open Data import API;Open Karelia system;geocontext markup;Geology;Data integration},
  doi = {10.1109/AINL-ISMW-FRUCT.2015.7382981},
  issn = {},
  month = nov,
  month_numeric = {11},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8029862,
  author = {{Choudhary}, S. and {Kimura}, K. and {Sekiguchi}, A.},
  booktitle = {2017 IEEE International Conference on Web Services (ICWS)},
  title = {SPEC2REST: An Approach for Eliciting Web API Resources from Existing Applications},
  year = {2017},
  volume = {},
  number = {},
  pages = {910-913},
  abstract = {Web API is a modern approach for exposing service data to use for applications, however, decision on Uniform Resource Identifiers (URIs) from an existing web application is still a manual and very time consuming task. Depending on the existing web application, thousands of lines of code has to be read and discussed to decide on what data can be exposed as web API resources. An automated approach is named SPEC2REST and proposed here for eliciting web API resources which uses class relations for path elicitation and filters web API resources using word occurrence. Evaluation results showed that SPEC2REST can elicit around 90% of actual existing web APIs for four applications by using class relations, as well as, helps inexperienced developers at their first step of creating RESTful resources.},
  keywords = {application program interfaces;Internet;Web services;web API resources;SPEC2REST;path elicitation;RESTful resources;Web API resources;Uniform Resource Identifiers;URI;Web pages;Twitter;Uniform resource locators;Unified modeling language;Companies;Documentation;Tools;RESTful API;Web API;Class diagram;URL;Automation},
  doi = {10.1109/ICWS.2017.119},
  issn = {},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7816483,
  author = {{Pandita}, R. and {Taneja}, K. and {Williams}, L. and {Tung}, T.},
  booktitle = {2016 IEEE International Conference on Software Maintenance and Evolution (ICSME)},
  title = {ICON: Inferring Temporal Constraints from Natural Language API Descriptions},
  year = {2016},
  volume = {},
  number = {},
  pages = {378-388},
  abstract = {Temporal constraints of an Application Programming Interface (API) are the allowed sequences of method invocations in the API governing the secure and robust operation of client software using the API. These constraints are typically described informally in natural language API documents, and therefore are not amenable to existing constraint-checking tools. Manually identifying and writing formal temporal constraints from API documents can be prohibitively time-consuming and error-prone. To address this issue, we propose ICON: an approach based on Machine Learning (ML) and Natural Language Processing (NLP) for identifying and inferring formal temporal constraints. To evaluate our approach, we use ICON to infer and formalize temporal constraints from the Amazon S3 REST API, the PayPal Payment REST API, and the java.io package in the JDK API. Our results indicate that ICON can effectively identify temporal constraint sentences (from over 4000 human annotated API sentences) with the average 79.0% precision and 60.0% recall. Furthermore, our evaluation demonstrates that ICON achieves an accuracy of 70% in inferring 77 formal temporal constraints from these APIs.},
  keywords = {application program interfaces;learning (artificial intelligence);natural language processing;ICON;natural language API description;application program interfaces;temporal constraints;client software;constraint-checking tools;machine learning;ML;natural language processing;NLP;Amazon S3 REST API;PayPal Payment REST API;JDK API;Natural languages;Contracts;Tagging;Documentation;Semantics;Dictionaries;Syntactics;NLP;Temporal Specifications;API},
  doi = {10.1109/ICSME.2016.59},
  issn = {},
  month = oct,
  month_numeric = {10},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8719480,
  author = {{Hosono}, M. and {Washizaki}, H. and {Fukazawa}, Y. and {Honda}, K.},
  booktitle = {2018 25th Asia-Pacific Software Engineering Conference (APSEC)},
  title = {An Empirical Study on the Reliability of the Web API Document},
  year = {2018},
  volume = {},
  number = {},
  pages = {715-716},
  abstract = {The importance of APIs in software development, especially web APIs, has increased Developers read documentation, which is available on the internet, and use the corresponding APIs in their products. However, documentation occasionally contains mistakes. Such mistakes can confuse developers or lead to defects that lower the quality of the product. In this paper, we investigate the reliability of web APIs by extracting and comparing OpenAPI specifications from both the documentations and the results of the API calls. Almost half of the documentations are somehow unreliable. Mismatches between documentation and the response can be categorized into four types: 1) Undocumented Keys, 2) Dynamic Keys, 3) Unreturned Keys, and 4) Type Mismatched. This study will help developers design more reliable products.},
  keywords = {application program interfaces;document handling;Internet;software engineering;API calls;software development;Web API document;Developers read documentation;Internet;product quality;OpenAPI specifications;undocumented keys;dynamic keys;unreturned keys;type mismatched;Documentation;Indexes;Reliability engineering;Software reliability;Dictionaries;Histograms;Web API, REST API, microservices, API documentation, documentation evolution},
  doi = {10.1109/APSEC.2018.00103},
  issn = {2640-0715},
  month = dec,
  month_numeric = {12},
  mrs_inclusion_criteria={all},
}
@inproceedings{7839142,
  author = {{Kumar}, K. and {Bose}, J. and {Tripathi}, S.},
  booktitle = {2016 IEEE Annual India Conference (INDICON)},
  title = {A unified web interface for the internet of things},
  year = {2016},
  volume = {},
  number = {},
  pages = {1-6},
  abstract = {The Internet of Things (IoT) includes many applications, frameworks and protocols that are specific to the types or categories of devices that are controlled. The Web browser represents a widely used and available interface to access functionality and is present in most mobile devices. In this paper we present an integrated interface to control IoT connected devices through the medium of the Web browser. We describe a lightweight method to integrate an IoT interface with a Web browser with the help of various REST APIs and wrapper functions. We use features and technologies specific to the browser, such as bookmarks, authentication and offline mode to control devices directly through the browser instead of going through a centralized server. Our solution will enable any user to easily control any connected smart device via a web browser. We also describe the architecture of an EEG controlled IoT interface for the Web browser.},
  keywords = {application program interfaces;electroencephalography;graphical user interfaces;Internet of Things;online front-ends;unified Web interface;Internet of Things;Web browser;integrated interface;IoT connected device control;REST API;wrapper functions;bookmarks;authentication;offline mode;centralized server;connected smart device;EEG controlled IoT interface;Web Browser;Web of Things;Internet of Things;REST API;HTML},
  doi = {10.1109/INDICON.2016.7839142},
  issn = {2325-9418},
  month = dec,
  month_numeric = {12},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7092950,
  author = {{Slominski}, A. and {Muthusamy}, V. and {Khalaf}, R.},
  booktitle = {2015 IEEE International Conference on Cloud Engineering},
  title = {Building a Multi-tenant Cloud Service from Legacy Code with Docker Containers},
  year = {2015},
  volume = {},
  number = {},
  pages = {394-396},
  abstract = {In this paper we address the problem of migrating a legacy Web application to a cloud service. We develop a reusable architectural pattern to do so and validate it with a case study of the Beta release of the IBM Bluemix Workflow Service [1] (herein referred to as the Beta Workflow service). It uses Docker [2] containers and a Cloudant [3] persistence layer to deliver a multi-tenant cloud service by re-using a legacy codebase. We are not aware of any literature that addresses this problem by using containers.The Beta Workflow service provides a scalable, stateful, highly available engine to compose services with REST APIs. The composition is modeled as a graph but authored in a Javascript-based domain specific language that specifies a set of activities and control flow links among these activities. The primitive activities in the language can be used to respond to HTTP REST requests, invoke services with REST APIs, and execute Javascript code to, among other uses, extract and construct the data inputs and outputs to external services, and make calls to these services.Examples of workflows that have been built using the service include distributing surveys and coupons to customers of a retail store [1], the management of sales requests between a salesperson and their regional managers, managing the staged deployment of different versions of an application, and the coordinated transfer of jobs among case workers.},
  keywords = {application program interfaces;cloud computing;Java;specification languages;Javascript code;HTTP REST requests;Javascript-based domain specific language;REST API;Cloudant persistence layer;Beta Workflow service;IBM Bluemix Workflow Service;reusable architectural pattern;legacy Web application;docker containers;legacy codebase;multitenant cloud service;Containers;Engines;Security;Cloud computing;Organizations;Browsers;Memory management},
  doi = {10.1109/IC2E.2015.66},
  issn = {},
  month = mar,
  month_numeric = {3},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8029806,
  author = {{Luo}, Y. and {Puyang}, T. and {Sun}, X. and {Shen}, Q. and {Yang}, Y. and {Ruan}, A. and {Wu}, Z.},
  booktitle = {2017 IEEE International Conference on Web Services (ICWS)},
  title = {RestSep: Towards a Test-Oriented Privilege Partitioning Approach for RESTful APIs},
  year = {2017},
  volume = {},
  number = {},
  pages = {548-555},
  abstract = {At present, a growing number of web applications especially cloud computing systems employ representational state transfer (REST) API as the interface to expose their services for simplicity and clarity. For security purposes, service providers prefer to control the access to the provided interface based on the principle of least privilege. However, how to divide the administrative privileges remains a difficulty in practice. In this work, we simplify the privilege partitioning problem into a classification problem of RESTful functions, so the permission to call a category of functions can be granted to a specific administrator. We propose a RESTful API classification approach called RestSep based on genetic algorithm. A classification is represented as a 2-dimensional matrix, which is used as the chromosome. Customized operators of selection, mutation and crossover are designed. The fitness function is designed to balance parameters such as number of categories, test case coverage, function overlapping, etc. Experiments on popular clouds like OpenStack and Kubernetes indicate RestSep can generate a self-explanatory classification result, which can serve as a guideline for privilege partitioning. The overhead of test generation is at most 13.1% and the overhead of genetic algorithm is at most 183.29s, which are acceptable for practical use.},
  keywords = {application program interfaces;authorisation;cloud computing;genetic algorithms;matrix algebra;pattern classification;RestSep;test-oriented privilege partitioning approach;web applications;representational state transfer API;security purposes;service providers;administrative privileges;classification problem;RESTful functions;RESTful API classification approach;genetic algorithm;2-dimensional matrix;fitness function;test case coverage;function overlapping;self-explanatory classification result;test generation;cloud computing systems;mutation operator;crossover operator;access control;selection operator;Cloud computing;Genetic algorithms;Sociology;Statistics;Access control;representational state transfer;privilege partition;genetic algorithm;integration test},
  doi = {10.1109/ICWS.2017.64},
  issn = {},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7194337,
  author = {d. {Benedictis}, A. and {Rak}, M. and {Turtur}, M. and {Villano}, U.},
  booktitle = {2015 IEEE 24th International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises},
  title = {REST-Based SLA Management for Cloud Applications},
  year = {2015},
  volume = {},
  number = {},
  pages = {93-98},
  abstract = {In cloud computing, possible risks linked to availability, performance and security can be mitigated by the adoption of Service Level Agreements (SLAs) formally agreed upon by cloud service providers and their users. This paper presents the design of services for the management of cloud-oriented SLAs that hinge on the use of a REST-based API. Such services can be easily integrated into existing cloud applications, platforms and infrastructures, in order to support SLA-based cloud services delivery. After a discussion on the SLA life-cycle, an agreement protocol state diagram is introduced. It takes explicitly into account negotiation, remediation and renegotiation issues, is compliant with all the active standards, and is compatible with the WS-Agreement standard. The requirement analysis and the design of a solution able to support the proposed SLA protocol is presented, introducing the REST API used. This API aims at being the basis for a framework to build SLA-based applications.},
  keywords = {application program interfaces;cloud computing;contracts;diagrams;formal specification;formal verification;protocols;systems analysis;REST-based SLA management;REST-based API;service level agreement;cloud computing;cloud service provider;CSP;SLA-based cloud services delivery;agreement protocol state diagram;requirement analysis;Standards;Monitoring;XML;Cloud computing;Uniform resource locators;Security;Protocols;Cloud;SLA;WS-Agreement;REST;API},
  doi = {10.1109/WETICE.2015.36},
  issn = {1524-4547},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8878524,
  author = {{Hartina}, D. A. and {Lawi}, A. and {Panggabean}, B. L. E.},
  booktitle = {2018 2nd East Indonesia Conference on Computer and Information Technology (EIConCIT)},
  title = {Performance Analysis of GraphQL and RESTful in SIM LP2M of the Hasanuddin University},
  year = {2018},
  volume = {},
  number = {},
  pages = {237-240},
  abstract = {GraphQL is a new concept in building an API. GraphQL is a Query Language developed by Facebook and implemented on the server side. Although it is a query language, the GraphQL is not directly connected with the database. In other words, GraphQL is not limited to both SQL and NOSQL databases. GraphQL which uses single endpoints is more efficient than RESTful which uses many endpoints but GraphQL will also be a little slower in querying complex databases and have many relationships beside that REST is built on multiple endpoints for specifying the return data, oftentimes multiple endpoints be required to be called when it needed. It will increase the number of client-server calls for displaying the data to the user and this could possibly result in poorer performance of the service in a Web Application needs. This paper analyses the performance calculation of the GraphQL and RESTful technologies in the web information services system of the Institute for Research and Community Service (LP2M) of the Hasanuddin University. The performance parameters used are Response Time and Throughput. Our results showed that in terms of speed RESTful is still superior to the GraphQL since the speed of RESTful is consistently stable in terms of access time and data size. Whereas the GraphQL is dynamic since it can be change depend on demand fluctuation.},
  keywords = {application program interfaces;client-server systems;database management systems;Internet;query languages;query processing;relational databases;social networking (online);SQL;GraphQL;query language;RESTful;SIM LP2M;Hasanuddin University;NOSQL databases;complex databases;Web application needs;Web information services system;Institute for Research and Community Service;Web services;Time factors;Throughput;Databases;Quality of service;Database languages;Testing;GraphQL;RESTful;API;throughput;response time},
  doi = {10.1109/EIConCIT.2018.8878524},
  issn = {},
  month = nov,
  month_numeric = {11},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7543742,
  author = {{Mulfari}, D. and {Celesti}, A. and {Fazio}, M. and {Villari}, M. and {Puliafito}, A.},
  booktitle = {2016 IEEE Symposium on Computers and Communication (ISCC)},
  title = {Using Google Cloud Vision in assistive technology scenarios},
  year = {2016},
  volume = {},
  number = {},
  pages = {214-219},
  abstract = {Google Cloud Vision is an image recognition technology that allows us to remotely process the content of an image and to retrieve its main features. By using specialized REST API, called Google Cloud Vision API, developers exploit such a technology within their own applications. Currently, this tool is in limited preview and its services are accessible for trusted tester users only. From a developer's perspective, in this paper, we intend to use such software resources in order to achieve assistive technology solutions for people with disabilities. Specifically, we investigate some potential benefits of Cloud Vision tool towards the development of applications for users who are blind.},
  keywords = {application program interfaces;assisted living;cloud computing;computer vision;feature extraction;handicapped aids;image recognition;information retrieval;image recognition;remote image content processing;feature retrieval;REST API;Google Cloud Vision API;software resources;disabled people;assistive technology solutions;Cloud Vision tool;Google;Cloud computing;Cameras;Feature extraction;Hardware;Visualization;Assistive Technology;Google Cloud Vision;API;Image Processing;Embedded Systems},
  doi = {10.1109/ISCC.2016.7543742},
  issn = {},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7558065,
  author = {{Luo}, Y. and {Zhou}, H. and {Shen}, Q. and {Ruan}, A. and {Wu}, Z.},
  booktitle = {2016 IEEE International Conference on Web Services (ICWS)},
  title = {RestPL: Towards a Request-Oriented Policy Language for Arbitrary RESTful APIs},
  year = {2016},
  volume = {},
  number = {},
  pages = {666-671},
  abstract = {Recently an increasing number of web applications especially cloud computing systems utilize representational state transfer (REST) API to deploy their services for simplicity and clarity. Users can employ the same interface to invoke various applications from the Internet. For security purposes, service providers would control the access to the provided interface through policy enforcement. Yet the access control of REST interfaces lacks a uniform standard regarding the policy language and corresponding enforcement implementation, which brings two limitations: i) Users have to deal with totally different types of policies to accommodate certain systems. ii) Service providers have to design their own platform-specific authorization policy language and the related enforcement mechanisms. In this paper, we propose a REST Policy Language (RestPL) to express the authorization policies especially for REST APIs. RestPL is ensured to be request-oriented, based on our definition of the standard request form. This indicates that a RestPL policy can be automatically generated from an actual request, which helps mitigate a user's pressure during policy designing. Furthermore, we also provide a reference implementation for the enforcement code of RestPL based on regular expressions and deploy it on OpenStack Liberty to demonstrate its feasibility. The experimental results indicate the enforcement overhead of RestPL can be reduced to 80.6% compared with the original policy. In addition, we show that an end-user can also benefit from RestPL for reducing the learning effort by at least 41.6%.},
  keywords = {application program interfaces;authorisation;formal languages;Web services;RestPL;request-oriented policy language;RESTful API;Web applications;cloud computing systems;representational state transfer;Internet;security purposes;service providers;access control;policy enforcement;REST interfaces;authorization policy language;REST policy language;regular expressions;OpenStack Liberty;Cloud computing;Standards;Authorization;Grammar;representational state transfer;access control;security policy;request-oriented},
  doi = {10.1109/ICWS.2016.92},
  issn = {},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, api_spec, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose},
}
@inproceedings{7755381,
  author = {{Thacker}, U. and {Pandey}, M. and {Rautaray}, S. S.},
  booktitle = {2016 International Conference on Electrical, Electronics, and Optimization Techniques (ICEEOT)},
  title = {Performance of elasticsearch in cloud environment with nGram and non-nGram indexing},
  year = {2016},
  volume = {},
  number = {},
  pages = {3624-3628},
  abstract = {The fact that technology have changed the lives of human beings cannot be denied. It has drastically reduced the effort needed to perform a particular task and has increased the productivity and efficiency. Computers especially have been playing a very important role in almost all fields in today's world. They are used to store large amount of data in almost all sectors, be it business and industrial sectors, personal lives or any other. The research areas of science and technology uses computers to solve complex and critical problems. Information is the most important requirement of each individual. In this era of quick-growing and huge data, it has become increasingly illogical to analyse it with the help of traditional techniques or relational databases. New big data instruments, architectures and designs have come into existence to give better support to the requirements of organizations/institutions in analysing large data. Specifically, Elasticsearch, a full-text java based search engine, designed keeping cloud environment in mind solves issues of scalability, search in real time, and efficiency that relational databases were not able to address. In this paper, we present our own experience with Elasticsearch an open source, Apache Lucene based, full-text search engine that provides near real-time search ability, as well as a RESTful API for the ease of user in the field of research.},
  keywords = {application program interfaces;Big Data;cloud computing;data analysis;Java;search engines;ngram indexing;RESTful API;full-text search engine;Apache Lucene;open source;Java based search engine;Elasticsearch;data analysis;big data instruments;productivity;nonngram indexing;cloud environment;Indexing;Ports (Computers);Search problems;Computers;Java;Switches;Elasticsearch;Indexing;Restful},
  doi = {10.1109/ICEEOT.2016.7755381},
  issn = {},
  month = mar,
  month_numeric = {3},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{5676125,
  author = {{Cholia}, S. and {Skinner}, D. and {Boverhof}, J.},
  booktitle = {2010 Gateway Computing Environments Workshop (GCE)},
  title = {NEWT: A RESTful service for building High Performance Computing web applications},
  year = {2010},
  volume = {},
  number = {},
  pages = {1-11},
  abstract = {The NERSC Web Toolkit (NEWT) brings High Performance Computing (HPC) to the web through easy to write web applications. Our work seeks to make HPC resources more accessible and useful to scientists who are more comfortable with the web than they are with command line interfaces. The effort required to get a fully functioning web application is decreasing, thanks to Web 2.0 standards and protocols such as AJAX, HTML5, JSON and REST. We believe HPC can speak the same language as the web, by leveraging these technologies to interface with existing grid technologies. NEWT presents computational and data resources through simple transactions against URIs. In this paper we describe our approach to building web applications for science using a RESTful web service. We present the NEWT web service and describe how it can be used to access HPC resources in a web browser environment using AJAX and JSON. We discuss our REST API for NEWT, and address specific challenges in integrating a heterogeneous collection of backend resources under a single web service. We provide examples of client side applications that leverage NEWT to access resources directly in the web browser. The goal of this effort is to create a model whereby HPC becomes easily accessible through the web, allowing users to interact with their scientific computing, data and applications entirely through such web interfaces.},
  keywords = {application program interfaces;grid computing;Web services;NEWT Web service;NERSC Web toolkit;high performance computing;Web applications;Web 2.0 standards;Web 2.0 protocols;HTML5;JSON;REST;grid technologies;RESTful Web service;AJAX;Web browser environment;REST API;Web interfaces;scientific computing;Logic gates;Browsers;Authentication;Scientific computing;Protocols;Web services;Buildings;science gateways;scientific computing;REST;HTTP;web;grid;AJAX;JSON;HPC},
  doi = {10.1109/GCE.2010.5676125},
  issn = {2152-1093},
  month = nov,
  month_numeric = {11},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8904508,
  author = {{Ed-douibi}, H. and {Cánovas Izquierdo}, J. L. and {Bordeleau}, F. and {Cabot}, J.},
  booktitle = {2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)},
  title = {WAPIml: Towards a Modeling Infrastructure for Web APIs},
  year = {2019},
  volume = {},
  number = {},
  pages = {748-752},
  abstract = {Web APIs are becoming key assets for any business. Most of these Web APIs are "REST-like", meaning that they adhere partially to the Representational State Transfer (REST) architectural style. The OpenAPI Initiative (OAI) was launched with the objective of creating a vendor neutral, portable, and open specification for describing REST APIs. The initiative has succeeded in attracting major companies and the OpenAPI specification has become de facto format for describing REST APIs. However, there is currently a lack of tools to provide modeling facilities for developers who want to manage and visualize their OpenAPI definitions as models and integrate them into model-based processes. In this paper, we propose WAPIml an OpenAPI round-trip tool that leverages model-driven techniques to create, visualize, manage, and generate OpenAPI definitions. WAPIml embeds an OpenAPI metamodel but also an OpenAPI UML profile to enable working with Web APIs in any UML-compatible modeling tool.},
  keywords = {application program interfaces;software architecture;Unified Modeling Language;Web services;WAPIml;modeling infrastructure;Web API;Representational State Transfer architectural style;open specification;OpenAPI specification;modeling facilities;OpenAPI definitions;model-based processes;OpenAPI round-trip tool;leverages model-driven techniques;UML-compatible modeling tool;REST API;REST APIs;OpenAPI;UML;UML profile},
  doi = {10.1109/MODELS-C.2019.00116},
  issn = {},
  month = sep,
  month_numeric = {9},
  mrs_inclusion_criteria={full_version, recent, rest_api, api_spec, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose},
}
@inproceedings{6906837,
  author = {{Sabbouh}, M. and {McCracken}, K. and {Cooney}, G.},
  booktitle = {2014 IEEE International Congress on Big Data},
  title = {Data Sharing for Cloud Computing Platforms},
  year = {2014},
  volume = {},
  number = {},
  pages = {621-628},
  abstract = {Cloud computing platforms consist of a set of reliable services that are run in the cloud. Typically, consumer applications use software development kits (SDKs) provided by the computing platform services to store, update, and retrieve instances of data in the cloud. Services provided by the cloud computing platform, expose different data modeling paradigms that consumer applications use to interact with the cloud. The service-specific data modeling paradigms and SDKs increase the complexity of data sharing between consumer applications that interact with the different services of the cloud computing platform. To make matters more complicated, it's not uncommon in an enterprise to find different groups using different cloud computing platforms. In this paper, we will describe a set of abstractions that can be used to abstract different computing platforms. The abstractions not only abstract the computing platform, but also enable the data discovery and sharing between applications. We will further show that these abstractions do not add substantial latency on the performance of the computing platform.},
  keywords = {cloud computing;data analysis;data models;cloud computing platforms;consumer applications;software development kits;SDK;computing platform services;data instances;data retrieval;data update;data storage;service-specific data modeling paradigms;data sharing;data abstractions;data discovery;Electronics packaging;Data models;Protocols;Computational modeling;Distributed databases;Resource description framework;OWL;web protocols;data modeling language;NoSQL;Rest;API;JSON},
  doi = {10.1109/BigData.Congress.2014.95},
  issn = {2379-7703},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8452850,
  author = {{Sasirekha}, S. and {Paul}, I. J. L. and {Swamynathan}, S.},
  booktitle = {2018 International Conference on Computer, Communication, and Signal Processing (ICCCSP)},
  title = {An API Centric Smart Kitchen Application},
  year = {2018},
  volume = {},
  number = {},
  pages = {1-6},
  abstract = {With emerging technology, more automation is expected which can be used on a daily basis. In the future, most of the homes will be equipped with sensing and interaction services which would be used for monitoring, controlling and alarming giving users a new and a different experience. The aim of introducing smart technology in houses is for providing a better quality living to the residents by the integrating technology and services. These smart applications focus on safety, security, care and comfort while this paper focuses on comfort. One of the major issues today is to having many items in the kitchen but not knowing what to cook. This paper mainly addresses this issue by suggesting the appropriate recipes according to the food items present in the kitchen. Each food item is tagged using Radio-Frequency Identification (RFID) tag and is scanned using a RFID card reader. Based on the items scanned this application suggests a list of recipes that can be cooked from the food items available at that particular point of time. Design of Application Programming Interfaces (API) helps in reusability of modules across various applications. APIs minimize the need to understand every detail of the component that forms a building block by means of abstraction. The proposed paper is API centric since most of the functionality is accomplished through API calls thus realizing complete abstraction. The interaction between APIs is achieved through RESTful web services. Representational State Transfer (REST) is an architectural style which runs over HTTP(HyperText Transfer Protocol) provides light weight communication and when web services use REST architecture they are called RESTful APIs. Another reason for using RESTful web services is that they are stateless and hence easily scalable. The flexibility of REST makes it extremely useful in providing consumer need specific message payloads and also assigning resources their Universal Resource Identifier (URI). Therefore, the proposed paper aims to develop APIs for smart kitchen enabling easy extensibility to other similar applications like smart health.},
  keywords = {application program interfaces;home automation;hypermedia;radiofrequency identification;transport protocols;Web services;representational state transfer;API centric smart kitchen application;hypertext transfer protocol;web services;RESTful API;HTTP;application programming interfaces;radiofrequency identification tag;smart health;REST architecture;RFID card reader;food item;smart applications focus;smart technology;Web services;Radiofrequency identification;Databases;Signal processing;Representational state transfer;Libraries;Cooking;Recipe;Smart Kitchen;RFID;REST;URI;API},
  doi = {10.1109/ICCCSP.2018.8452850},
  issn = {},
  month = feb,
  month_numeric = {2},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@article{8828082,
  author = {{Bellotti}, F. and {Berta}, R. and {Paranthaman}, P. K. and {Dange}, G. R. and {De Gloria}, A.},
  journal = {IEEE Transactions on Games},
  title = {REAL: Reality-Enhanced AppLied-games},
  year = {2019},
  volume = {},
  number = {},
  pages = {1-1},
  abstract = {Pervasive games are an emerging genre combining reality and computing. This paper presents a suite of simple pervasive serious games we have developed to explore the concept of “reality-enhanced gaming”, a pattern to tie game play mechanics to the outcomes/measurements of real-world activities. The prototype games were realized in the context of TEAM, an industrial research project aimed at developing apps for flexible and collaborative mobility. The proposed games are examples of different UIs we considered useful to meet various significant scenarios, goals and user typologies, especially for improving car driving styles. Given the variety of information sources, contexts of use, and target users, we abstracted a gameoriented framework (namely REAL, Reality-Enhanced AppLied games), in order to support re-use and scalability. Through a set of RESTful APIs, the REAL framework separates sensor data from actual game implementations, so as to provide different experiences to users, according to their specific needs and preferences. This concept – which allows serious game developers to focus on their specific game logic while seamlessly exploiting a variety of field sensors – is general and may be applied to a variety of domains. We validated REAL developing and field testing five typologies of serious games. Subjective evaluation results show a good level of satisfaction and perceived usefulness. More tests are needed, especially in terms of different application contexts, impact on developers, number and variety of users, and exposure time. However, outcomes confirm the significant potential of reality-enhanced game design and the importance of tools for supporting their development.},
  keywords = {Games;Collaboration;Automobiles;Cloud computing;Prototypes;Fuels;Serious Games;Applied Games;IoT;RESTful API;Pervasive Games;Automotive},
  doi = {10.1109/TG.2019.2940108},
  issn = {2475-1510},
  month = {},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8668138,
  author = {{Eshetu}, Y. and {Bhuyan}, P. and {Behura}, A.},
  booktitle = {2018 International Conference on Communication, Computing and Internet of Things (IC3IoT)},
  title = {A Service-Oriented IoT System to Support Individuals with ASD},
  year = {2018},
  volume = {},
  number = {},
  pages = {251-256},
  abstract = {In this work, we are focusing on design and construction of Service-Oriented Architecture (SOA) enabled Internet of Things (IoT) based smart assistive system prototype, which can be dynamically configured and interfaced with array of sensors to prevent injuries of Autism Spectrum Disorder (ASD) individuals. This increase the quality of life of people with ASD. The proposed model suggested implementing a wearable array of sensor components to gather information about the physiological status of a person and environmental condition of his surroundings. Afterward the data forwarded to a cloud service for analysis purpose. The system compromises of various tier of interaction and storage, to form a ubiquitous platform. One of the layer in the tier are linked to wearable computing platform. Another layer of the tier is dedicated to data storage captured from raw sensor data. The interaction between the tiers nodes is performed over the Internet through HyperText Transfer Protocol (HTTP) REST API and Message Queuing Telemetry Transport (MQTT) protocol. The system work in three domain: services at home, services at community i.e. neighborhood and services away from home. We can access in real time and monitor of ASD person though mobile app and web. So our approach will be helpful to manage ASD affected individuals.},
  keywords = {application program interfaces;assisted living;handicapped aids;hypermedia;Internet of Things;mobile computing;sensor fusion;service-oriented architecture;telemetry;transport protocols;wearable computers;smart assistive system prototype;Autism Spectrum Disorder individuals;wearable array;sensor components;physiological status;environmental condition;cloud service;ubiquitous platform;wearable computing platform;data storage;HyperText Transfer Protocol REST API;Message Queuing Telemetry Transport protocol;ASD person;service-oriented architecture;sensor data;service-oriented IoT system;Internet of Things;Injuries;Internet of Things;Service-oriented architecture;Sensor arrays;Wearable sensors;Intelligent sensors;IoT;Smart environment;SOA;Wearable technology;ASD},
  doi = {10.1109/IC3IoT.2018.8668138},
  issn = {},
  month = feb,
  month_numeric = {2},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8501476,
  author = {{Pryss}, R. and {Schobel}, J. and {Reichert}, M.},
  booktitle = {2018 4th International Workshop on Requirements Engineering for Self-Adaptive, Collaborative, and Cyber Physical Systems (RESACS)},
  title = {Requirements for a Flexible and Generic API Enabling Mobile Crowdsensing mHealth Applications},
  year = {2018},
  volume = {},
  number = {},
  pages = {24-31},
  abstract = {Presently, mHealth becomes increasingly important in supporting patients in their everyday life. For example, diabetes patients can monitor themselves by the use of their smartphones. On the other, clinicians as well as medical researchers try to exploit the advantages of mobile technology. More specifically, mHealth applications can gather data in everyday life and are able to easily collect sensor or context data (e.g., the current temperature). Compared to clinical trials, these advantages enable mHealth applications to gather more data in a rather short time. Besides, humans often behave atypically in a clinical environment and, hence, mHealth applications collect data in a setting that reflects the daily behavior more naturally. Hitherto, many technical solutions emerged to deal with such data collection settings. Mobile crowdsensing is one prominent example in this context. We utilize the latter technology in a multitude of large-scale projects to gather data of several chronic disorders. In the TrackYourTinnitus project, for example, we pursue the goal to reveal new medical insights to the tinnitus disorder. We learned in the realized projects that a sophisticated API must be provided to cope with the requirements of researchers from the medical domain. Notably, the API must be able to flexibly deal with requirement changes. The work at hand presents the elicited requirements and illustrate the pillars on which our flexible and generic API is built on. Although we identified that the maintenance of such an API is a challenging endeavor, new data evaluation opportunities arise that are promising in the context of chronic disorder management.},
  keywords = {application program interfaces;data handling;health care;medical disorders;mobile computing;patient monitoring;sensor fusion;flexible API;generic API;data evaluation opportunities;diabetes patients;medical researchers;mobile technology;mHealth applications;data collection settings;mobile crowdsensing;patient monitoring;smartphones;sensor data;context data;clinical environment;large-scale projects;TrackYourTinnitus project;tinnitus disorder;chronic disorder management;Mobile applications;Androids;Humanoid robots;Diabetes;Mobile handsets;Stress;mHealth, mobile crowdsensing, mobile data collection, chronic disorder, RESTful API, requirements, flexibility},
  doi = {10.1109/RESACS.2018.00010},
  issn = {},
  month = aug,
  month_numeric = {8},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8710534,
  author = {{Belfadel}, A. and {Amdouni}, E. and {Laval}, J. and {Cherifi}, C. and {Moalla}, N.},
  booktitle = {2018 International Conference on Intelligent Systems (IS)},
  title = {Ontology-based Software Capability Container for RESTful APIs},
  year = {2018},
  volume = {},
  number = {},
  pages = {466-473},
  abstract = {Software reuse and REST-based Web Applications resulted from open initiatives become an interesting opportunity for companies to save effort, time and cost during the design and development of new business needs. Gather and qualify these services in a container helps to discover, match and reuse them in developing new business applications for companies. Our objective in this work is the design of a software capability container offering a wider view qualification for REST-based services. Moreover, we aim to enrich the designed container with semantic elements to ease the discovery and the selection of the qualified services. In this purpose, we define an ontology based on a proposed Enterprise Architecture Capability Profile offering a qualification covering business, operational and technical aspects for services. Ontologies are widely acknowledged as a means to specify explicitly the meaning of concepts in a domain of interest, and to facilitate consistent sharing of data and knowledge pertaining to them. The qualification profile is based on a proposed meta-model that helps to retrieve and gather initial requirements used to guide the development of existing REST-based Web Applications. Furthermore, a Framework is proposed to exploit the designed container in order to respond to users requirements for developing future business process and efficiently reuse the qualified services. Our contribution aims to upgrade technical components to the level of end-users requirements. This helps to accelerate business application development and improve the reuse and sustainability of existing services.},
  keywords = {application program interfaces;business data processing;meta data;ontologies (artificial intelligence);software architecture;software reusability;Web services;business application development;RESTful APIs;software reuse;open initiatives;REST-based services;operational aspects;ontology-based software capability container;REST-based Web applications;enterprise architecture capability profile;meta-model;end-users requirements;Business;Ontologies;Containers;Computer architecture;Service-oriented architecture;software reuse;capability container;software capability;ontology;Restful API;enterprise architecture;TOGAF;SOA;sustainability;business process;requirement analysis},
  doi = {10.1109/IS.2018.8710534},
  issn = {1541-1672},
  month = sep,
  month_numeric = {9},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{6844717,
  author = {{Rusk}, D. and {Coady}, Y.},
  booktitle = {2014 28th International Conference on Advanced Information Networking and Applications Workshops},
  title = {Location-Based Analysis of Developers and Technologies on GitHub},
  year = {2014},
  volume = {},
  number = {},
  pages = {681-685},
  abstract = {GitHub is a popular platform for collaboration on open source projects. It also provides a rich API to query various aspects of the public activity. This combination of a popular social coding website with a rich API presents an opportunity for researchers to gather empirical data about software development practices. There are an overwhelmingly large number of competing platforms to choose from in software development. Knowing which are gaining widespread adoption is valuable both for individual developers trying to increase their employability, as well as software engineers deciding which technology to use in their next big project. In terms of a developer's employability and an employer's ability to find available developers in their economic region, it is important to identify the most common technologies by geographic location. In this paper, analyses are done on GitHub data taking into account the developers' location and their technology usage. A web-based tool has been developed to interact with and visualize this data. In its current state of development, the tool summarizes the amount of code developers have in their public repositories broken down by programming language, and summarizes data about programmers using specific programming languages. This allows website visitors to get an immediate picture of the programming language usage in their region of interest. Future research could expand this work to technologies beyond programming languages such as frameworks and libraries.},
  keywords = {application program interfaces;data visualisation;mobile computing;programming languages;public domain software;query processing;software engineering;software tools;user interfaces;Web sites;location-based analysis;open source projects;rich API;public activity;social coding Web site;software development technology;employer ability;developer employability;economic region;geographic location;GitHub data;developer location;Web-based tool;data visualization;public repository;code developers;programming language;region of interest;Software;Computer languages;Data visualization;Electronic mail;Encoding;Educational institutions;Companies;GitHub;REST API;software repository;programming languages;open source},
  doi = {10.1109/WAINA.2014.110},
  issn = {},
  month = may,
  month_numeric = {5},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{6676758,
  author = {{Sellami}, M. and {Yangui}, S. and {Mohamed}, M. and {Tata}, S.},
  booktitle = {2013 IEEE Sixth International Conference on Cloud Computing},
  title = {PaaS-Independent Provisioning and Management of Applications in the Cloud},
  year = {2013},
  volume = {},
  number = {},
  pages = {693-700},
  abstract = {The study we have conducted of existing cloud platforms shows that their operating requires the use of specific and proprietary APIs. This PaaS providers' policy is hampering the interactions between different clouds. If appropriate solutions are not considered, this issue would for instance slow down the democratization of clouds federation and cooperation. In this paper, we propose (i) a unified description model that allows the representation of applications independently of the targeted PaaS for their hosting and (ii) a generic PaaS application provisioning and management API (called COAPS API). Our proposed solution applies the separation of concerns principle by separating the provisioning and the management API from the defined description model. We motivate our solution with real use case scenarios and an implementation to show its feasibility.},
  keywords = {application program interfaces;cloud computing;PaaS-independent provisioning;cloud platforms;proprietary API;PaaS providers policy;clouds federation;clouds cooperation;unified description model;PaaS application provisioning;PaaS application management;COAPS API;Containers;Computational modeling;Companies;Databases;Cloud computing;Foundries;Application model;Environment model;PaaS;Provisioning;Management;REST API},
  doi = {10.1109/CLOUD.2013.105},
  issn = {2159-6190},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{6928910,
  author = {{Li}, C. and {Zhang}, R. and {Huai}, J. and {Sun}, H.},
  booktitle = {2014 IEEE International Conference on Web Services},
  title = {A Novel Approach for API Recommendation in Mashup Development},
  year = {2014},
  volume = {},
  number = {},
  pages = {289-296},
  abstract = {Mashing up Web services and RESTful APIs is a novel programming approach to develop new applications. As the number of available resources is increasing rapidly, to discover potential services or APIs is getting difficult. Therefore, it is vital to relieve mashup developers of the burden of service discovery. In this paper, we propose a probabilistic model to assist mashup creators by recommending a list of APIs that may be used to compose a required mashup given descriptions of the mashup. Specifically, a relational topic model is exploited to characterize the relationship among mashups, APIs and their links. In addition, we incorporate the popularity of APIs to the model and make predictions on the links between mashups and APIs. Moreover, the statistical analysis on a public mashup platform shows the current status of mashup development and the applicability of this study. Experiments on a large service data set confirm the effectiveness of this proposed approach.},
  keywords = {application program interfaces;probability;recommender systems;statistical analysis;Web services;Web services;RESTful API;programming approach;service discovery;probabilistic model;API list recommendation;relational topic model;mashup relationship characterization;statistical analysis;public mashup platform;mashup development;large service data set;Mashups;Vectors;Mathematical model;Data models;Probabilistic logic;Google;mashup development;API recommendation;relational topic model},
  doi = {10.1109/ICWS.2014.50},
  issn = {},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{6081079,
  author = {{Blum}, N. and {Yamada}, J. and {Uchida}, N. and {Magedanz}, T.},
  booktitle = {2011 15th International Conference on Intelligence in Next Generation Networks},
  title = {Enabling information push for network- and device-agnostic internet services},
  year = {2011},
  volume = {},
  number = {},
  pages = {225-230},
  abstract = {In a multi-network environment, users are able to consume services via several networks, protocols, and connected devices. These services are usually bound to their specific technology and often limited to a dedicated device or service provider domain. In this paper, we investigate information distribution via push versus the client-server-based request/response model against the background of a converged Internet and Telecoms service stratum on top of multiple access networks. In our approach, convergence is achieved by integrating multiple separated communication channels in the service layer on top of an IP Multimedia Subsystem (IMS) based Telecom environment and by providing an Identity enabler for interactions of the platform with 3rd parties. We propose a service architecture enabling information sharing and push notifications via an abstract RESTful API for multiple communication channels.},
  keywords = {access protocols;client-server systems;Internet;multimedia systems;telecommunication channels;telecommunication services;network-agnostic Internet services;device-agnostic Internet services;information push;multinetwork environment;protocols;connected devices;service provider domain;information distribution;client-server;request/response model;telecoms service stratum;multiple access networks;IP multimedia subsystem;RESTful API;multiple communication channels;Authentication;Message service;Protocols;Portals;Electronic mail;Mobile handsets;Distributed information systems;Multi-access communication;Push services},
  doi = {10.1109/ICIN.2011.6081079},
  issn = {},
  month = oct,
  month_numeric = {10},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7377677,
  author = {{Achchuthan}, Y. and {Sarveswaran}, K.},
  booktitle = {2015 Fifteenth International Conference on Advances in ICT for Emerging Regions (ICTer)},
  title = {Language localisation of Tamil using Statistical Machine Translation},
  year = {2015},
  volume = {},
  number = {},
  pages = {125-129},
  abstract = {Language localisation, where the strings in interface and documentation are translated to a new language, is a rigorous and time consuming task. On the other hand machine translation systems, specifically Statistical Machine Translation (SMT) systems, are successfully used among many language pairs. A few SMT systems have been developed for generic domain; however, there are no systems available to aid localisation yet. This research proposes a new methodology in which language localisation can be done using SMT. This research also identifies suitable parameters on which a SMT aided localisation system could be built. A pilot system is developed and the system is also outlined in this paper. A RESTful API has also been developed to facilitate localisation in remote tools. Several open source software have been translated already to Tamil. Those translated English - Tamil pairs were collected from various language resource files and then cleaned, tokenised and were used to train the system. Another similar system is prepared with data from generic domain apart from the collected technical data. Systems were trained with 2-gram, 3-gram and 4-gram language models that are created using two different language modelling tools namely KenLM and IRSTLM. Then the results were evaluated using BLEU algorithm. Appropriate parameters for setting up SMT system for localisation were identified from the evaluation. The results show that it would be enough to train a system with 3-gram, and the modified BLEU algorithm will give better understanding of the results compare to the original implementation of it. Further KenLM was found to perform better than IRSTM in terms of accuracy of results and the speed of execution.},
  keywords = {application program interfaces;language translation;natural language processing;Tamil language localisation;statistical machine translation;SMT aided localisation system;RESTful API;application program interface;open source software;KenLM modelling tool;IRSTLM modelling tool;BLEU algorithm;Google;SMT;Localisation;Language Modelling;BLEU;Tamil},
  doi = {10.1109/ICTER.2015.7377677},
  issn = {},
  month = aug,
  month_numeric = {8},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8057358,
  author = {{Alabbas}, W. and {al-Khateeb}, H. M. and {Mansour}, A. and {Epiphaniou}, G. and {Frommholz}, I.},
  booktitle = {2017 International Conference On Social Media, Wearable And Web Analytics (Social Media)},
  title = {Classification of colloquial Arabic tweets in real-time to detect high-risk floods},
  year = {2017},
  volume = {},
  number = {},
  pages = {1-8},
  abstract = {Twitter has eased real-time information flow for decision makers, it is also one of the key enablers for Open-source Intelligence (OSINT). Tweets mining has recently been used in the context of incident response to estimate the location and damage caused by hurricanes and earthquakes. We aim to research the detection of a specific type of high-risk natural disasters frequently occurring and causing casualties in the Arabian Peninsula, namely `floods'. Researching how we could achieve accurate classification suitable for short informal (colloquial) Arabic text (usually used on Twitter), which is highly inconsistent and received very little attention in this field. First, we provide a thorough technical demonstration consisting of the following stages: data collection (Twitter REST API), labelling, text pre-processing, data division and representation, and training models. This has been deployed using `R' in our experiment. We then evaluate classifiers' performance via four experiments conducted to measure the impact of different stemming techniques on the following classifiers SVM, J48, C5.0, NNET, NB and k-NN. The dataset used consisted of 1434 tweets in total. Our findings show that Support Vector Machine (SVM) was prominent in terms of accuracy (F1=0.933). Furthermore, applying McNemar's test shows that using SVM without stemming on Colloquial Arabic is significantly better than using stemming techniques.},
  keywords = {classification;data mining;disasters;floods;Internet;learning (artificial intelligence);natural language processing;social networking (online);support vector machines;text analysis;text pre-processing;data division;classifiers SVM;colloquial Arabic tweets;real-time information flow;decision makers;Open-source Intelligence;OSINT;tweets mining;incident response;hurricanes;earthquakes;high-risk natural disasters;Arabian Peninsula;short informal Arabic text;data collection;Twitter REST API;stemming techniques;Twitter;high-risk floods;Twitter;Support vector machines;Floods;Event detection;Real-time systems;Niobium;Training;Arabic text classification;big data;Colloquialism;Event detection;Twitter;Real-time;Stemming;SVM},
  doi = {10.1109/SOCIALMEDIA.2017.8057358},
  issn = {},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{6468291,
  author = {{Francesco}, M. D. and {Li}, N. and {Raj}, M. and {Das}, S. K.},
  booktitle = {2012 IEEE International Conference on Green Computing and Communications},
  title = {A Storage Infrastructure for Heterogeneous and Multimedia Data in the Internet of Things},
  year = {2012},
  volume = {},
  number = {},
  pages = {26-33},
  abstract = {The Internet of Things (IoT) consists of networked objects deployed worldwide and connected over the Internet. As a consequence, the major aspects of the IoT are represented by the heterogeneity and the huge number of the participating devices. These aspects also constitute the major challenges in the definition of a storage infrastructure suitable for IoT applications. In this paper, we introduce a novel data model and storage infrastructure for the IoT to address these challenges. Different from other works in the literature, we exploit a document-oriented approach and show how it is suitable to support both heterogeneous and multimedia data. Our solution is built on top of the CouchDB database server, offers a Restful API, and provides a rich set of features targeted to IoT applications. Moreover, we devise optimized schemes for uploading documents which are specifically tailored to resource-constrained IoT devices. We evaluate our proposed schemes both analytically and with experiments in a real system.},
  keywords = {application program interfaces;document handling;Internet of Things;multimedia systems;storage management;storage infrastructure;heterogeneous data;multimedia data;Internet of Things;IoT application;networked object;document-oriented approach;CouchDB database server;Restful API;Databases;Sensors;Multimedia communication;Internet;Data models;Authentication;Software;Storage;heterogeneous and multimedia data;document-oriented database;Restful;Internet of Things},
  doi = {10.1109/GreenCom.2012.15},
  issn = {},
  month = nov,
  month_numeric = {11},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7943369,
  author = {{Tseng}, Y. and {Zhang}, Z. and {Naït-Abdesselam}, F.},
  booktitle = {2016 17th International Conference on Parallel and Distributed Computing, Applications and Technologies (PDCAT)},
  title = {ControllerSEPA: A Security-Enhancing SDN Controller Plug-in for OpenFlow Applications},
  year = {2016},
  volume = {},
  number = {},
  pages = {268-273},
  abstract = {Software-defined networking (SDN), as a new network paradigm, has the advantage of centralizing control and global visibility over a network. However, security issues remain a major concern and prevent SDN from being widely adopted. One of the challenges is the prevention of malicious OpenFlow application (OF app) access to the SDN controller as it opens a programmable northbound interface for third party applications. In this paper, we address app-to-control security issues with focus on five main attack vectors: unauthorized access, illegal function calling, malicious rules injection, resources exhausting and manin-the-middle attack. Based on the identified threat models, we develop a light-weight plug-in, which is called ControllerSEPA, by using RESTful API to defend SDN controller against malicious OF apps. Specifically, ControllerSEPA can provide the services including OF app-based AAA control (unlike OpenDaylight and ONOS which offer user-based or role-based AAA control), rule conflict resolution, OF app isolation, fine-grained access control and encryption. Furthermore, we study the feasibility of deploying ControllerSEPA on five open source SDN controllers: OpenDaylight, ONOS, Floodlight, Ryu and POX. Results show that the deployment operates with very low complexity, and most of time the modification of source codes is unnecessary. In our implementations, the repacked services in ControllerSEPA create negligible latency (0.1% to 0.3%) and can provide more rich services to OF apps.},
  keywords = {application program interfaces;authorisation;computer network security;cryptography;software defined networking;ControllerSEPA;security-enhancing SDN controller plug-in;software-defined networking;malicious OpenFlow application;programmable northbound interface;third party applications;app-to-control security issues;attack vectors;unauthorized access;illegal function calling;malicious rules injection;resources exhausting;man-in-the-middle attack;RESTful API;OF app-based AAA control;rule conflict resolution;OF app isolation;fine-grained access control;encryption;open source SDN controllers;OpenDaylight;ONOS;Floodlight;Ryu;POX;Monitoring;Control systems;Process control;Encryption;Authentication;Authorization;SDN controller;OpenFlow applications;security},
  doi = {10.1109/PDCAT.2016.064},
  issn = {},
  month = dec,
  month_numeric = {12},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8395218,
  author = {{Würsch}, M. and {Liwicki}, M. and {Ingold}, R.},
  booktitle = {2018 13th IAPR International Workshop on Document Analysis Systems (DAS)},
  title = {Web Services in Document Image Analysis - Recent Developments on DIVAServices and the Importance of Building an Ecosystem},
  year = {2018},
  volume = {},
  number = {},
  pages = {334-339},
  abstract = {Web Services are being adapted into the workflows of many Document Image Analysis researchers. However, so far, there is no common platform for providing access to algorithms in the community. DIVAServices aims to become this by providing a platform that is open to the whole community to provide their own methods as Web Services. In this paper we present updates and enhancements made to the existing DIVAServices platform. This includes a new computational backend, a revamped execution workflow based on asynchronous communication, and the possibility for methods to specify their outputs. Furthermore, we discuss the importance of an ecosystem for such platforms. We argue that only providing a RESTful API is not enough. Users need tools and services around the framework that support them in adapting the Web Services and we introduce some of the tools that we built around DIVAServices.},
  keywords = {application program interfaces;document image processing;ecology;Web services;workflow management software;Web services;ecosystem;DIVAServices platform;document image analysis researchers;computational backend;revamped execution workflow;asynchronous communication;restful api;tools;Web services;Tools;Text analysis;Ecosystems;Libraries;Servers;Image analysis;Web Services;Document Image Analysis;Ecosystem},
  doi = {10.1109/DAS.2018.40},
  issn = {},
  month = apr,
  month_numeric = {4},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8093000,
  author = {{Verdugo}, P. and {Salvachiua}, J. and {Huecas}, G.},
  booktitle = {2017 56th FITCE Congress},
  title = {An agile container-based approach to TaaS},
  year = {2017},
  volume = {},
  number = {},
  pages = {10-15},
  abstract = {Current cloud deployment scenarios imply a need for fast testing of user oriented software in diverse, heterogeneous and often unknown hardware and network environments, making it difficult to ensure optimal or reproducible in-site testing. The current paper proposes the use of container based lightweight virtualization with a ready-to-run, just-intime deployment strategy in order to minimize time and resources needed for streamlined multicomponent prototyping in PaaS systems. To that end, we will study a specific case of use consisting of providing end users with pre-tested custom prepackaged and preconfigured software, guaranteeing the viability of the aforementioned custom software, the syntactical integrity of the provided deployment system, the availability of needed dependencies as well as the sanity check of the already deployed and running software. From an architectural standpoint, by using standard, common use deployment packages as Chef or Puppet hosted in parallellizable workloads over ready-to-run Docker images, we can minimize the time required for full-deployment multicomponent systems testing and validation, as well as wrap the commonly provided features via a user-accessible RESTful API. The proposed infrastructure is currently available and freely accessible as part of the FIWARE EU initiative, and is open to third party collaboration and extension from a FOSS perspective.},
  keywords = {application program interfaces;cloud computing;program testing;public domain software;service-oriented architecture;software packages;software prototyping;virtualisation;fast testing;user oriented software;network environments;container based lightweight virtualization;just-intime deployment strategy;streamlined multicomponent prototyping;PaaS systems;syntactical integrity;sanity check;deployed running software;standard use deployment packages;common use deployment packages;ready-to-run Docker images;full-deployment multicomponent systems testing;user-accessible RESTful API;custom software;agile container-based approach;cloud deployment scenarios;TaaS;often unknown hardware environments;reproducible in-site testing;optimal in-site testing;time minimization;resources minimization;pretested custom preconfigured software;pretested custom prepackaged software;Chef;Puppet;full-deployment multicomponent systems validation;Testing;Cloud computing;Virtualization;Hardware;Containers;Standards},
  doi = {10.1109/FITCE.2017.8093000},
  issn = {},
  month = sep,
  month_numeric = {9},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7433072,
  author = {{Oberhauser}, R.},
  booktitle = {2015 8th International Conference on Advanced Software Engineering Its Applications (ASEA)},
  title = {A Hypermedia-Driven Approach for Adapting Processes via Adaptation Processes},
  year = {2015},
  volume = {},
  number = {},
  pages = {73-80},
  abstract = {In addition to an increasing need for business process management systems (BPMS) to dynamically adapt to situational change, there is increased interest in web service accessibility of BPMS to broaden their integration in enterprises and the cloud. Towards this end, a RESTful hypermedia-driven enactment and adaptation of processes has not been adequately explored. This paper thus investigates a hypermedia extension of our process adaptation approach AProPro we call AProProh (Adapting Processes via Processes using hypermedia) that dynamically provides hypermedia to guide process clients in the navigation, enactment, and adaptation of process instances. Clients of process-aware information systems (PAIS) have hitherto been tied to vendor-specific APIs and lacked a standard web-based API. The AProProh HATEOAS-PAIS middleware enables process clients to be more generic and PAIS-agnostic via REST APIs. Additionally, process clients can dynamically apply valid adaptations or adjust to process model changes utilizing dynamically generated hypermedia. Based on a case study prototype realization supporting dynamic invocation of process adaptation patterns, the initial evaluation results show its feasibility and gauge its performance overhead.},
  keywords = {business process re-engineering;workflow management software;hypermedia-driven approach;adaptation process;business process management system;BPMS;process-aware information systems;PAIS-agnostic;REST API;workflow management systems;Web-based API;Adaptation models;Business process management;Web services;Navigation;Runtime;Artificial intelligence;Adaptive process-aware information systems;PAIS;adaptive workflow management systems;WfMS;RESTful web services;hypermedia;HATEOAS;business process management systems;BPMS;dynamic usiness process management;aspectoriented processes;change patter},
  doi = {10.1109/ASEA.2015.22},
  issn = {},
  month = nov,
  month_numeric = {11},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{6864375,
  author = {{Saaranen}, M. and {Parak}, J. and {Honko}, H. and {Aaltonen}, T. and {Korhonen}, I.},
  booktitle = {IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)},
  title = {W2E — Wellness Warehouse Engine for semantic interoperability of consumer health data},
  year = {2014},
  volume = {},
  number = {},
  pages = {350-354},
  abstract = {Novel health monitoring devices and applications allow consumers easy and ubiquitous ways to monitor their health status. However, technologies from different providers lack both technical and semantic interoperability and hence the resulting health data is often deeply tied to specific service, which is limiting its re-usability and utilization in different services. We have designed a Wellness Warehouse Engine (W2E) that bridges this gap and enables seamless exchange of data between different services. W2E provides interfaces to various data sources and makes data available via unified REST API to other services. Importantly, it includes Unifier - an engine that allows transforming input data into generic units reusable by other services, and Analyzer - an engine that allows advanced analysis of input data, such as combining different data sources into new output parameters. In this paper, we describe the architecture of W2E and demonstrate its applicability by using it for uniting data from several consumer activity trackers. Finally, we discuss challenges of building Unifier and Analyzer engines for ever-enlarging number of new devices.},
  keywords = {data warehouses;electronic health records;open systems;consumer activity trackers;analyzer engine;reusable generic units;unifier engine;unified REST API;W2E;consumer health data;semantic interoperability;wellness warehouse engine;Engines;Calibration;Semantics;Interoperability;Educational institutions;Monitoring;Energy resolution},
  doi = {10.1109/BHI.2014.6864375},
  issn = {2168-2208},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7195591,
  author = {{Chen}, Y. and {Wang}, J. and {Wang}, H. and {Huang}, S. and {Lin}, C.},
  booktitle = {2015 IEEE International Conference on Web Services},
  title = {COSS: Content-Based Subscription as an IoT Service},
  year = {2015},
  volume = {},
  number = {},
  pages = {369-376},
  abstract = {Publish/subscribe (pub/sub) systems are widely used in numerous Internet-Of-Things (IoT) applications such as environment monitoring, supply chain tracing, healthcare, and vehicle networks. In these applications, publishers (e.g. Smart devices, sensors) are continuously generating large volume of data with an extremely high throughput, whereas subscribers are only interested in a small portion of the data. Recently, content-based subscription systems have raised more and more attentions by the researchers where subscribers can specify rules on the content of messages that are composed of many attributes. For example, in traffic monitoring, an operator is only interested in the data within a specified area defined by constraints on latitude and longitude instead of the whole map. In this paper, we present COSS, the first Content-based Subscription Service for IoT with natural multi-tenant support and easy-to-use REST APIs. Moreover, we investigate in the problem of Balanced Rule Engine Partitioning for content-based subscription under the Tenant-Message-Rule (TMR) model. We show the NP-hardness of the problem and design a heuristics to enable COSS to adaptively adjust the message distribution according to the workload history, and to scale on both the high data throughput of IoT workloads and multi-tenant. Extensive experiments show that COSS offers high performance and scalability for content-based subscription in terms of the number of tenants, and the data throughput of the messages.},
  keywords = {application program interfaces;content-based retrieval;Internet of Things;middleware;optimisation;Web services;COSS;IoT service;Internet-Of-Things;environment monitoring;supply chain tracing;healthcare;vehicle networks;content-based subscription system;traffic monitoring;content-based subscription service;REST API;balanced rule engine partitioning;tenant-message-rule model;TMR model;NP-hard problem;message distribution;middleware;Web service;Engines;Servers;Throughput;Tunneling magnetoresistance;Filtering;Temperature sensors;Web service;IoT data management;content-based subscription;scalability},
  doi = {10.1109/ICWS.2015.56},
  issn = {},
  month = jun,
  month_numeric = {6},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{6032453,
  author = {{McIntosh}, S. and {Adams}, B. and {Nguyen}, T. H. D. and {Kamei}, Y. and {Hassan}, A. E.},
  booktitle = {2011 33rd International Conference on Software Engineering (ICSE)},
  title = {An empirical study of build maintenance effort},
  year = {2011},
  volume = {},
  number = {},
  pages = {141-150},
  abstract = {The build system of a software project is responsible for transforming source code and other development artifacts into executable programs and deliverables. Similar to source code, build system specifications require maintenance to cope with newly implemented features, changes to imported Application Program Interfaces (APIs), and source code restructuring. In this paper, we mine the version histories of one proprietary and nine open source projects of different sizes and domain to analyze the overhead that build maintenance imposes on developers. We split our analysis into two dimensions: (1) Build Coupling, i.e., how frequently source code changes require build changes, and (2) Build Ownership, i.e., the proportion of developers responsible for build maintenance. Our results indicate that, despite the difference in scale, the build system churn rate is comparable to that of the source code, and build changes induce more relative churn on the build system than source code changes induce on the source code. Furthermore, build maintenance yields up to a 27% overhead on source code development and a 44% overhead on test development. Up to 79% of source code developers and 89% of test code developers are significantly impacted by build maintenance, yet investment in build experts can reduce the proportion of impacted developers to 22% of source code developers and 24% of test code developers.},
  keywords = {application program interfaces;software maintenance;software management;build maintenance effort;software project;application program interfaces;source code restructuring;API;build coupling;build ownership;source code development;Maintenance engineering;Software;Java;Couplings;Linux;Libraries;build systems;empirical software engineering;mining software repositories},
  doi = {10.1145/1985793.1985813},
  issn = {1558-1225},
  month = may,
  month_numeric = {5},
  mrs_inclusion_criteria={full_version, recent, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec, not_rest_api},
}
@inproceedings{8234464,
  author = {{Wang}, B. and {Rosenberg}, D. and {Boehm}, B. W.},
  booktitle = {2017 IEEE 28th Annual Software Technology Conference (STC)},
  title = {Rapid realization of executable domain models via automatic code generation},
  year = {2017},
  volume = {},
  number = {},
  pages = {1-6},
  abstract = {The gap between design and implementation always exists because changes happen frequently throughout software development process, along with rapid release cycles, and accompanied by time constraints and limited resources. The focus of our work is to reduce this gap for service-oriented projects. We proposed an approach which considers both technical strategies and agile methods, trying to streamline the progression from design to implementation at a relatively early phase, and then throughout the whole development lifecycle. Automatic code generation has the potential to reduce above problems to a certain extent. This paper describes our efforts to enable rapid and continuous delivery while leveraging parallelism in development via automatic code generation - specifically making domain models instantly executable. We describe a code generator that has been built to enable parallel development of services. It uses UML class diagram to model the problem domain, then rapidly realize the domain model as a set of NoSQL database collections, automate the generation of common database access functions, and automate the wrapping of these database functions within a set of RESTful APIs. We also consider several common deployment scenarios (e.g. requirements for media-handling, security, scalability) to ensure the flexibility and reusability of the target source code for subsequent development iterations. Several empirical project instances have been built using this code generation technique. Combine with agile methods, we attempt to shorten development schedule in both design and implementation stages, and to eliminate the risks caused by evolutionary development. The result shows a great saving of effort on development and less issues in implementation stage.},
  keywords = {application program interfaces;database management systems;program compilers;service-oriented architecture;software prototyping;Unified Modeling Language;development lifecycle;automatic code generation;rapid delivery;continuous delivery;code generator;parallel development;problem domain;common database access functions;target source code;subsequent development iterations;code generation technique;agile methods;development schedule;implementation stage;evolutionary development;executable domain models;software development process;rapid release cycles;service-oriented projects;UML class diagram;NoSQL database collections;RESTful API;Unified modeling language;Tools;Electronic mail;NoSQL databases;Service-oriented architecture;Adaptation models;rapid delivery;continuous delivery;NoSQL;REST;code generation;service-oriented architecture;UML modeling},
  doi = {10.1109/STC.2017.8234464},
  issn = {},
  month = sep,
  month_numeric = {9},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7917102,
  author = {{Ueta}, K. and {Xue}, X. and {Nakamoto}, Y. and {Murakami}, S.},
  booktitle = {2016 IEEE International Conference on Internet of Things (iThings) and IEEE Green Computing and Communications (GreenCom) and IEEE Cyber, Physical and Social Computing (CPSCom) and IEEE Smart Data (SmartData)},
  title = {A Distributed Graph Database for the Data Management of IoT Systems},
  year = {2016},
  volume = {},
  number = {},
  pages = {299-304},
  abstract = {The Internet of Things(IoT) has become a popular technology, and various middleware has been proposed and developed for IoT systems. However, there have been few studies on the data management of IoT systems. In this paper, we consider graph database models for the data management of IoT systems because these models can specify relationships in a straightforward manner among entities such as devices, users, and information that constructs IoT systems. However, applying a graph database to the data management of IoT systems raises issues regarding distribution and security. For the former issue, we propose graph database operations integrated with REST APIs. For the latter, we extend a graph edge property by adding access protocol permissions and checking permissions using the APIs with authentication. We present the requirements for a use case scenario in addition to the features of a distributed graph database for IoT data management to solve the aforementioned issues, and implement a prototype of the graph database.},
  keywords = {application program interfaces;distributed databases;graph theory;Internet of Things;distributed graph database;data management;IoT systems;Internet of Things;middleware;graph database operations;REST API;graph edge property;access protocol permissions;Conferences;Internet of Things;Green computing;Social computing;IoT;distributed system;graph database;data management;access control},
  doi = {10.1109/iThings-GreenCom-CPSCom-SmartData.2016.74},
  issn = {},
  month = dec,
  month_numeric = {12},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7856724,
  author = {{Santos}, T. and {Serrão}, C.},
  booktitle = {2016 11th International Conference for Internet Technology and Secured Transactions (ICITST)},
  title = {Secure Javascript Object Notation (SecJSON) Enabling granular confidentiality and integrity of JSON documents},
  year = {2016},
  volume = {},
  number = {},
  pages = {329-334},
  abstract = {Currently, web and mobile-based systems exchange information with other services, mostly through APIs that extend the functionality and enable multipart interoperable information exchange. Most of this is accomplished through the usage of RESTful APIs and data exchange that is conducted using JSON over the HTTP or HTTPS protocol. In the case of the exchange requires some specific security requirements, SSL/TLS protocol is used to create a secure authenticated channel between the two communication end-points. This is a scenario where all the content of the channels is encrypted and is useful if the sender and the receptor are the only communicating parties, however this may not be the case. The authors of this paper, present a granular mechanism for selectively offering confidentiality and integrity to JSON documents, through the usage of public-key cryptography, based on the mechanisms that have been used also to provide XML security. The paper presents the proposal of the syntax for the SecJSON mechanism and an implementation that was created to offer developers the possibility to offer this security mechanism into their own services and applications.},
  keywords = {application program interfaces;data integrity;document handling;electronic data interchange;Internet;mobile computing;open systems;public key cryptography;transport protocols;XML;secure Javascript object notation;SecJSON mechanism;granular JSON document confidentiality;granular JSON document integrity;Web system;mobile-based system;multipart interoperable information exchange;RESTful API;data exchange;HTTP protocol;HTTPS protocol;security requirements;SSL protocol;TLS protocol;secure authenticated channel;communication end-points;public-key cryptography;XML security;security mechanism;Cryptography;XML;Instant messaging;World Wide Web;Protocols;Encoding;Security;Integrity;Confidentiality;API;JSON;HTTPS;SSL/TLS},
  doi = {10.1109/ICITST.2016.7856724},
  issn = {},
  month = dec,
  month_numeric = {12},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{8609598,
  author = {{Samuel}, H. and {Noori}, B. and {Farazi}, S. and {Zaiane}, O.},
  booktitle = {2018 IEEE/WIC/ACM International Conference on Web Intelligence (WI)},
  title = {Context Prediction in the Social Web Using Applied Machine Learning: A Study of Canadian Tweeters},
  year = {2018},
  volume = {},
  number = {},
  pages = {230-237},
  abstract = {In this ongoing work, we present the Grebe social data aggregation framework for extracting geo-fenced Twitter data for analysis of user engagement in health and wellness topics. Grebe also provides various visualization tools for analyzing temporal and geographical health trends. Grebe currently has over 18 million indexed public tweets, and is the first of its kind for Canadian researchers. The large dataset is used for analyzing three types of contexts: geographical context via prediction of user location using supervised learning, topical context via determining health-related tweets using various learning approaches, and affective context via sentiment analysis of tweets using rule-based methods. For the first, we define user location as the position from which users are posting a tweet and use standard precision metrics for evaluation with promising results for predicting provinces and cities from tweet text. For the second, we use a broader definition of health using the six dimensions of wellness model and evaluate using manually annotated documents with good results using supervised and semi-supervised machine learning. For the third, we use the indexed tweets to show current trends in emotions and opinions and demonstrate trends in polarity and emotions across various Canadian provinces. The combination of these contexts provides useful insights for digital epidemiology. Ultimately, the vision of Grebe is to provide researchers with Canada-specific social web datasets through an open source platform with an accessible RESTful API, and this paper showcases Grebe's potential and presents our progress towards achieving these goals.},
  keywords = {knowledge based systems;learning (artificial intelligence);medical information systems;sentiment analysis;social networking (online);text analysis;context prediction;Canadian tweeters;Grebe social data;geo-fenced Twitter data;user engagement;wellness topics;visualization tools;geographical health trends;geographical context;user location;supervised learning;topical context;health-related tweets;learning approaches;affective context;sentiment analysis;rule-based methods;tweet text;wellness model;manually annotated documents;semisupervised machine learning;Canadian provinces;Canada-specific social web datasets;indexed public tweets;Twitter;Diseases;Market research;Tools;Supervised learning;Standards;Data aggregation;location prediction;health social media;sentiment analysis;big data;Twitter},
  doi = {10.1109/WI.2018.00-85},
  issn = {},
  month = dec,
  month_numeric = {12},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@inproceedings{7269340,
  author = {{Edwards}, T. and {Keane}, B.},
  booktitle = {SMPTE 2014 Annual Technical Conference Exhibition},
  title = {Can COTS Ethernet Switches Handle Uncompressed Video?},
  year = {2014},
  volume = {},
  number = {},
  pages = {1-14},
  abstract = {The carriage of real-time video over Ethernet networks promises significant benefits to the broadcast industry. But there has been some concern about whether commerical-off-the-shelf (COTS) Ethernet switches can meet broadcast quality of service (QoS) requirements. This paper describes the results of a range of static and dynamic tests of Ethernet switches using packet flows that are representative of uncompressed HD video, looking specifically at packet loss, packet reordering, latency, and packet delay variation (PDV). Flow test generators and analyzers include a new flexible FPGA architecture controlled using a RESTful API.},
  keywords = {Networked Media;2022-6;Ethernet},
  doi = {10.5594/M001543},
  issn = {},
  month = oct,
  month_numeric = {10},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
@article{7118237,
  author = {{Sellami}, R. and {Bhiri}, S. and {Defude}, B.},
  journal = {IEEE Transactions on Services Computing},
  title = {Supporting Multi Data Stores Applications in Cloud Environments},
  year = {2016},
  volume = {9},
  number = {1},
  pages = {59-71},
  abstract = {The production of huge amount of data and the emergence of cloud computing have introduced new requirements for data management. Many applications need to interact with several heterogeneous data stores depending on the type of data they have to manage: traditional data types, documents, graph data from social networks, simple key-value data, etc. Interacting with heterogeneous data models via different APIs, and multiple data store applications imposes challenging tasks to their developers. Indeed, programmers have to be familiar with different APIs. In addition, the execution of complex queries over heterogeneous data models cannot, currently, be achieved in a declarative way as it is used to be with mono-data store application, and therefore requires extra implementation efforts. Moreover, developers need to master and deal with the complex processes of cloud discovery, and application deployment and execution. In this paper we propose an integrated set of models, algorithms and tools aiming at alleviating developers task for developing, deploying and migrating multiple data stores applications in cloud environments. Our approach focuses mainly on three points. First, we provide a unifying data model used by applications developers to interact with heterogeneous relational and NoSQL data stores. Based on that, they express queries using OPEN-PaaS-DataBase API (ODBAPI), a unique REST API allowing programmers to write their applications code independently of the target data stores. Second, we propose virtual data stores, which act as a mediator and interact with integrated data stores wrapped by ODBAPI. This run-time component supports the execution of single and complex queries over heterogeneous data stores. Finally, we present a declarative approach that enables to lighten the burden of the tedious and non-standard tasks of (1) discovering relevant cloud environment and (2) deploying applications on them while letting developers to simply focus on specifying their storage and computing requirements. A prototype of the proposed solution has been developed and is currently used to implement use cases from the OpenPaaS project.},
  keywords = {application program interfaces;cloud computing;query processing;relational databases;cloud environments;cloud computing;data management;heterogeneous data models;API;complex queries execution;migrating multiple data stores applications;unifying data model;heterogeneous relational data stores;NoSQL data stores;OPEN-PaaS-DataBase API;ODBAPI;declarative approach;OpenPaaS project;Data models;Databases;Algebra;Cloud computing;Big data;Prototypes;REST-based API;NoSQL data stores;relational data stores;join queries;polyglot persistence;manifest based matching;REST-based API;NoSQL data stores;relational data stores;join queries;polyglot persistence;manifest based matching},
  doi = {10.1109/TSC.2015.2441703},
  issn = {1939-1374},
  month = jan,
  month_numeric = {1},
  mrs_inclusion_criteria={full_version, recent, rest_api, cs_journal},
  mrs_exclusion_criteria={irrelevant_purpose, not_api_spec},
}
